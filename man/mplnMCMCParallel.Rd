% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mplnMCMCEMClustering.R
\name{mplnMCMCParallel}
\alias{mplnMCMCParallel}
\title{Clustering Using MPLN With MCMC-EM Via Parallel Performance}
\usage{
mplnMCMCParallel(
  dataset,
  membership = "none",
  gmin = 1,
  gmax = 2,
  nChains = 3,
  nIterations = 1000,
  initMethod = "kmeans",
  nInitIterations = 0,
  normalize = "Yes",
  numNodes = NA
)
}
\arguments{
\item{dataset}{A dataset of class matrix and type integer such that
rows correspond to observations and columns correspond to variables.
The dataset have dimensions n x d, where n is the total number of
observations and d is the dimensionality. If rowSums are zero,
these rows will be removed prior to cluster analysis.}

\item{membership}{A numeric vector of length nrow(dataset) containing the
cluster membership of each observation. If not available,
leave as "none".}

\item{gmin}{A positive integer specifying the minimum number of components
to be considered in the clustering run.}

\item{gmax}{A positive integer, >= gmin, specifying the maximum number of
components to be considered in the clustering run.}

\item{nChains}{A positive integer specifying the number of Markov chains.
Default is 3, the recommended minimum number.}

\item{nIterations}{A positive integer specifying the number of iterations
for each MCMC chain (including warmup). The value should be greater than
40. The upper limit will depend on size of dataset.}

\item{initMethod}{An algorithm for initialization. Current options are
"kmeans", "random", "medoids", "clara", or "fanny". Default is "kmeans".}

\item{nInitIterations}{A positive integer or zero, specifying the number
of initialization runs to be performed. This many runs, each with 10
iterations, will be performed via MPLNClust and values from the run with
highest log-likelihood will be used as initialization values. Default is 0.}

\item{normalize}{A string with options "Yes" or "No" specifying
if normalization should be performed. Currently, normalization factors
are calculated using TMM method of edgeR package. Default is "Yes".}

\item{numNodes}{A positive integer indicating the number of nodes to be
used from the local machine to run the clustering algorithm. Else
leave as NA, so default will be detected as
parallel::makeCluster(parallel::detectCores() - 1).}
}
\value{
Returns an S3 object of class mplnMCMCParallel with results.
\itemize{
\item dataset - The input dataset on which clustering is performed.
\item dimensionality - Dimensionality of the input dataset.
\item normalizationFactors - A vector of normalization factors used
for input dataset.
\item gmin - Minimum number of components considered in the clustering
run.
\item gmax - Maximum number of components considered in the clustering
run.
\item initalizationMethod - Method used for initialization.
\item allResults - A list with all results.
\item logLikelihood - A vector with value of final log-likelihoods for
each cluster size.
\item numbParameters - A vector with number of parameters for each
cluster size.
\item trueLabels - The vector of true labels, if provided by user.
\item ICLresults - A list with all ICL model selection results.
\item BICresults - A list with all BIC model selection results.
\item AICresults - A list with all AIC model selection results.
\item AIC3results - A list with all AIC3 model selection results.
\item slopeHeuristics - If more than 10 models are considered, slope heuristic
results.
\item DjumpModelSelected - If more than 10 models are considered, slope heuristic
results.
\item DDSEModelSelected - If more than 10 models are considered, slope heuristic
results.
\item totalTime - Total time used for clustering and model selection.
}
}
\description{
Performs clustering using mixtures of multivariate Poisson-log
normal (MPLN) distribution with Markov chain Monte Carlo
expectation-maximization algorithm (MCMC-EM) for parameter
estimation. Coarse grain parallelization is employed, such that
when a range of components/clusters (g = 1,...,G) are considered, each
G is run on a different processor. This can be performed because
each component/cluster size is independent from another. All
components/clusters in the range to be tested have been parallelized
to run on a seperate core using the \emph{parallel} R package. The number of
nodes to be used for clustering can be specified or calculated using
\emph{parallel::detectCores() - 1}. Model selection is performed using
AIC, AIC3, BIC and ICL.
}
\examples{
# Generating simulated data
# Not run
# trueMu1 <- c(6.5, 6, 6, 6, 6, 6)
# trueMu2 <- c(2, 2.5, 2, 2, 2, 2)

# trueSigma1 <- diag(6) * 2
# trueSigma2 <- diag(6)

# sampleData <- mplnDataGenerator(nObservations = 100,
#                                 dimensionality = 6,
#                                 mixingProportions = c(0.79, 0.21),
#                                 mu = rbind(trueMu1, trueMu2),
#                                 sigma = rbind(trueSigma1, trueSigma2),
#                                 produceImage = "No")

# Clustering
# mplnResults <- mplnMCMCParallel(dataset = sampleData$dataset,
#                                 membership = sampleData$trueMembership,
#                                 gmin = 1,
#                                 gmax = 2,
#                                 nChains = 3,
#                                 nIterations = 1000,
#                                 initMethod = "kmeans",
#                                 nInitIterations = 2,
#                                 normalize = "Yes")

}
\references{
Aitchison, J. and C. H. Ho (1989). The multivariate Poisson-log normal distribution.
\emph{Biometrika} 76.

Akaike, H. (1973). Information theory and an extension of the maximum likelihood
principle. In \emph{Second International Symposium on Information Theory}, New York, NY,
USA, pp. 267–281. Springer Verlag.

Biernacki, C., G. Celeux, and G. Govaert (2000). Assessing a mixture model for
clustering with the integrated classification likelihood. \emph{IEEE Transactions
on Pattern Analysis and Machine Intelligence} 22.

Bozdogan, H. (1994). Mixture-model cluster analysis using model selection criteria
and a new informational measure of complexity. In \emph{Proceedings of the First US/Japan
Conference on the Frontiers of Statistical Modeling: An Informational Approach:
Volume 2 Multivariate Statistical Modeling}, pp. 69–113. Dordrecht: Springer Netherlands.

Schwarz, G. (1978). Estimating the dimension of a model. \emph{The Annals of Statistics}
6.

Silva, A. et al. (2019). A multivariate Poisson-log normal mixture model
for clustering transcriptome sequencing data. \emph{BMC Bioinformatics} 20.
\href{https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-019-2916-0}{Link}
}
\author{
{Anjali Silva, \email{anjali.silva@uhnresearch.ca}, Sanjeena Dang,
\email{sdang@math.binghamton.edu}. }
}
